"""
@author: pythonpanda2 (aka G.S.)
"""
import jax
import jax.numpy as jnp
import equinox as eqx
import json
#import sentencepiece as spm
from tokenizer import MistralTokenizer
from model import Transformer
from typing import Optional, Tuple, List, NamedTuple
from jax import random, grad, jit
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
from preprocess_Suzuki_Coupling_data import make_reaction 
from rope import precompute_frequencies
import os
import torch  
from torch.utils.data import Dataset, DataLoader
import numpy as np 
import optax


# Mistral-7B Model setting
class ModelArgs(NamedTuple):
    dim: int
    n_layers: int
    n_heads: int
    n_kv_heads: int
    head_dim: int
    hidden_dim: int
    vocab_size: int
    sliding_window: int
    norm_eps: float
    max_batch_size: int = 32


#Load the pre-trained Mistral model, tokenize inputs
class Mistral7B:
    def __init__(self, mistralpath, key, dtype=jnp.bfloat16):
        self.mistralpath = mistralpath
        self.mistral_param_path = os.path.join(mistralpath, 'params.json')
        self.mistral_pretrained_path = os.path.join(mistralpath, 'mistral7B_jax_port_fast.eqx')
        self.tokenizer_path = os.path.join(mistralpath, 'tokenizer.model')
        self.sp = MistralTokenizer(self.tokenizer_path)
        with open(self.mistral_param_path, "r") as f:
            self.args = ModelArgs(**json.loads(f.read()))
        self.mistral_model = Transformer(self.args, key, dtype)
        self.mistral_model = eqx.tree_deserialise_leaves(self.mistral_pretrained_path , self.mistral_model)

    #def get_mistral_pretrained(self):
        #return eqx.tree_deserialise_leaves(self.mistral_pretrained_path , self.mistral_model)

    def tokenize_smiles(self, smiles):
        return self.sp.encode(smiles)

    def _precompute(self,max_len):
        cache_k = jnp.zeros((self.args.max_batch_size, self.args.n_layers, self.args.sliding_window, self.args.n_kv_heads, self.args.head_dim), dtype=jnp.bfloat16)
        cache_v = jnp.zeros((self.args.max_batch_size, self.args.n_layers, self.args.sliding_window, self.args.n_kv_heads, self.args.head_dim), dtype=jnp.bfloat16)
        cos_freq, sin_freq = precompute_frequencies(self.args.head_dim, max_len)
        positions = jnp.arange(0, max_len)
        positions_padded = jnp.pad( positions, (0, self.args.sliding_window - len(positions)), constant_values=-1)
        return cache_k, cache_v, cos_freq, sin_freq, positions_padded


# Define the regression block with a Multi-head attention layer
class MultiHeadAttentionRegression(eqx.Module):
    rope_embeddings: eqx.nn.RotaryPositionalEmbedding
    mha: eqx.nn.MultiheadAttention
    ffn: eqx.nn.Sequential
    rmsnorm1: eqx.nn.RMSNorm
    rmsnorm2: eqx.nn.RMSNorm
    rmsnorm3: eqx.nn.RMSNorm
    rmsnorm4: eqx.nn.RMSNorm
    dropout: eqx.nn.Dropout
    linear1: eqx.nn.Linear
    linear2: eqx.nn.Linear
    output_layer: eqx.nn.Linear

    def __init__(self, num_heads, embed_dim, key):

        assert embed_dim > 0, "embed_dim must be a positive integer"
        
        # Split key for reproducibility
        subkey_mha, subkey_ffn,  subkey_linear1, subkey_linear2, subkey_output = jax.random.split(key, 5)
        
        self.rope_embeddings = eqx.nn.RotaryPositionalEmbedding(embedding_size=embed_dim // num_heads, dtype=jnp.float32)
        self.mha = eqx.nn.MultiheadAttention(num_heads=num_heads, query_size=embed_dim, dtype=jnp.float32,  key=subkey_mha)
        self.ffn = eqx.nn.MLP(
            embed_dim,
            out_size=embed_dim,
            width_size= embed_dim,
            depth=2,
            activation = jax.nn.silu,
            key=subkey_ffn,
        )
        self.rmsnorm1 = eqx.nn.RMSNorm(shape=embed_dim,  dtype=jnp.float32)
        self.rmsnorm2 = eqx.nn.RMSNorm(shape=embed_dim,  dtype=jnp.float32)
        self.dropout = eqx.nn.Dropout(p=0.1)
        self.linear1 = eqx.nn.Linear(embed_dim, 1024, key=subkey_linear1, dtype=jnp.float32)
        self.rmsnorm3 = eqx.nn.RMSNorm(shape=1024,  dtype=jnp.float32)
        self.linear2 = eqx.nn.Linear(1024, 256, key=subkey_linear2, dtype=jnp.float32)
        self.rmsnorm4 = eqx.nn.RMSNorm(shape=256,  dtype=jnp.float32)
        self.output_layer = eqx.nn.Linear(256, 1, key=subkey_output,  dtype=jnp.float32)

    def __call__(self, x, key=None, is_training=True):
        #x = mistral(x)
        
        if is_training:
            if key is None:
                raise ValueError("Dropout requires a key when running in training mode.")
            # Split key to use different keys for different dropout calls
            subkey_dropout_mha, subkey_dropout1, subkey_dropout2, subkey_dropout3 = jax.random.split(key, 4)

        def process_heads(
            query_heads: jnp.ndarray,
            key_heads: jnp.ndarray,
            value_heads: jnp.ndarray
        ) -> tuple[
            jnp.ndarray,
            jnp.ndarray,
            jnp.ndarray
        ]:
            query_heads = jax.vmap(self.rope_embeddings,
                                   in_axes=1,
                                   out_axes=1)(query_heads)
            key_heads = jax.vmap(self.rope_embeddings,
                                 in_axes=1,
                                 out_axes=1)(key_heads)

            return query_heads, key_heads, value_heads

        mha_out = self.mha(x, x, x, process_heads=process_heads)
        if is_training:
            mha = self.dropout(mha_out, key=subkey_dropout_mha)
        x =  x + mha_out  # Residual connection

        x = jax.vmap(self.rmsnorm1)(x) #Layer norm

        ff = jax.vmap(self.ffn)(x) #MLP
        if is_training:
            ff = self.dropout(ff, key=subkey_dropout1)
        x = x + ff   #Residual
        x = jax.vmap(self.rmsnorm2)(x)  #Layer norm

        #(Optional : Drop out after-->) Linear-->  BatchNorm-->  Swish blocks
        x = jax.vmap(self.linear1)(x) 
        if is_training:
            x = self.dropout(x, key=subkey_dropout2)
        x = jax.vmap(self.rmsnorm3)(x)
        x = jax.nn.silu(x)

        x = jax.vmap(self.linear2)(x) 
        if is_training:
            x = self.dropout(x, key=subkey_dropout3)
        x = jax.vmap(self.rmsnorm4)(x)
        x = jax.nn.silu(x)

        return jax.vmap(self.output_layer)(x)  # Output a scalar value

class ReactionDataset(Dataset):
    def __init__(self, rxn, yields):
        self.rxn = rxn
        self.yields = yields

    def __len__(self):
        return len(self.rxn)

    def __getitem__(self, idx):
        return self.rxn[idx], self.yields[idx]

#Test these classes below!
key=jax.random.PRNGKey(0)
path="/eagle/FOUND4CHEM/project/CL_4_RXN/CL_MISTRAL7B_REACT/model_files"

Mistral = Mistral7B(path,key) #<---- Class
#mistral_model = Mistral.get_mistral_pretrained() #<---- Actual model weights!
#Mistral.mistral_model

# Data Loading and Preprocessing
data_file  = "/eagle/FOUND4CHEM/project/CL_4_RXN/CL_MISTRAL7B_REACT/data/Suzuki-Miyaura/aap9112_Data_File_S1.xlsx"
df = make_reaction(data_file) #df[['rxn', 'y']]
#tokenized_smiles, yields, max_len = preprocess_data(df)

tokenized_rxn = [Mistral.tokenize_smiles(smiles) for smiles in df['rxn']]
max_len =     max(len(sublist) for sublist in tokenized_rxn) #len(tokenized_rxn[0])
padded_rxn = [sublist + [0] * (max_len - len(sublist)) for sublist in tokenized_rxn]
tokenized_padded_inp_array = np.stack([np.array(aa) for aa in padded_rxn]) # jnp.array(padded_a)
yields = np.array(df['y'], dtype=np.float32)

cache_k, cache_v, cos_freq, sin_freq, positions_padded = Mistral._precompute(max_len)

# 80/20 split
train_rxn, val_rxn, train_yields, val_yields = train_test_split(
    tokenized_padded_inp_array, yields, test_size=0.2, random_state=42
)

print(train_rxn.shape, type(train_rxn),  val_rxn.shape,  type(val_rxn) )
# Convert JAX arrays to PyTorch tensors
train_rxns_torch = torch.tensor(train_rxn)
train_yields_torch = torch.tensor(train_yields, dtype=torch.float32)
val_rxns_torch = torch.tensor(val_rxn)
val_yields_torch = torch.tensor(val_yields, dtype=torch.float32)

# Create dataset objects for training and validation
train_dataset = ReactionDataset(train_rxns_torch, train_yields_torch)
val_dataset = ReactionDataset(val_rxns_torch, val_yields_torch)

print("\n Batch size:",Mistral.args.max_batch_size)
train_loader = DataLoader(train_dataset, batch_size=Mistral.args.max_batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=Mistral.args.max_batch_size, shuffle=True)

# Initialize the regression block
embed_dim = 4096  # Assuming embedding dimension of Mistral
num_heads = 4  # Number of  attention head for regression task
predictor = MultiHeadAttentionRegression(num_heads, embed_dim,  key)

print("\n Detected Device: ",jax.devices())

# Step 1: Fine-tune the model
@eqx.filter_value_and_grad
def loss_fn(predictor, model,  batch_rxns, batch_yields, cache_k, cache_v, cos_freq, sin_freq, positions_padded ):

    batch_rxns = jnp.array(batch_rxns.numpy())
    batch_yields = jnp.array(batch_yields.numpy())
   
    model = jax.vmap(model, in_axes= (0, None, None, None, None, 0, 0))
    embeddings, _, _ = model(batch_rxns, cos_freq, sin_freq, positions_padded, None, cache_k, cache_v)
    #print("\nEmbedding shape: ",embeddings.shape)
    #predictions = jax.vmap(predictor)(embeddings)

    # Pass is_training=True during training
    predictions = jax.vmap(lambda emb: predictor(emb, key, is_training=True))(embeddings)

    loss = jnp.mean((predictions - batch_yields) ** 2)
    
    return loss

#Step 2 : Training step 
@eqx.filter_jit
def train_step(predictor, model, batch_rxns, batch_yields, cache_k, cache_v, cos_freq, sin_freq, positions_padded, opt_state):
    # Get loss and gradients
    loss, grads = loss_fn(predictor, model, batch_rxns, batch_yields, cache_k, cache_v, cos_freq, sin_freq, positions_padded)
    # Update the parameters using the optimizer
    updates, opt_state = optimizer.update(grads, opt_state, predictor)
    predictor = eqx.apply_updates(predictor, updates)
    
    return loss, predictor, opt_state


# Step 3: Initialize optimizer
optimizer = optax.adamw(learning_rate=1e-5)
opt_state = optimizer.init(eqx.filter(predictor, eqx.is_array))  # optimizer.init(predictor)

num_epochs = 2
print("\n Total number of train  batches : ",len(train_loader))
# Training Loop with DataLoader
for epoch in range(num_epochs):
    running_loss = 0.0
    step = 1 
    for batch_rxn, batch_yields in train_loader:
        # Perform a training step
        loss, predictor, opt_state  = train_step( predictor, Mistral.mistral_model,  batch_rxn, batch_yields, cache_k, cache_v, cos_freq, sin_freq, positions_padded, opt_state)
        
        loss = loss.item()
        print(f"step={step}, loss={loss}")
        # Accumulate loss for monitoring
        running_loss += loss
        step += 1 
    # Print epoch loss
    print(f"Epoch {epoch}, Loss: {running_loss / len(train_loader)}")

@eqx.filter_jit
def compute_val(predictor, model,  val_rxns, cache_k, cache_v, cos_freq, sin_freq, positions_padded ):
    model = jax.vmap(model, in_axes= (0, None, None, None, None, 0, 0))
    embeddings, _, _ = model(val_rxns, cos_freq, sin_freq, positions_padded, None, cache_k, cache_v)
    #predictions = jax.vmap(predictor)(embeddings)

    # Pass is_training=False during validation/inference
    predictions = jax.vmap(lambda emb: predictor(emb, None, is_training=False))(embeddings)

    return predictions

# Define the plotting function
def plot_true_vs_predicted(true_labels, predicted_labels, rmse, r2):
    plt.figure(figsize=(10, 6))
    plt.scatter(true_labels, predicted_labels, color='b', alpha=0.6, s=10, label='Predicted vs True')
    plt.plot([true_labels.min(), true_labels.max()], [true_labels.min(), true_labels.max()], 'k--', lw=2)
    plt.xlabel('True Yield', fontsize=12)
    plt.ylabel('Predicted Yield', fontsize=12)
    plt.title('True vs Predicted Yield', fontsize=14)
    plt.legend([f'RMSE: {rmse:.2f}\nR²: {r2:.2f}'], loc='upper left')
    plt.grid(True)
    plt.savefig("true_vs_pred_yield.png", dpi=500, bbox_inches='tight')


def validate(predictor, model,  val_loader, cache_k, cache_v, cos_freq, sin_freq, positions_padded ):
    running_val_loss = 0.0
    running_r2_score = 0.0
    all_predictions = []
    all_true_labels = []

    step = 1 
    for val_rxns, val_yields in val_loader:
        val_rxns = jnp.array(val_rxns.numpy())
        val_yields = jnp.array(val_yields.numpy())

        # Calculate embeddings and predictions without updating the model
        predictions = compute_val(predictor, model,  val_rxns, cache_k, cache_v, cos_freq, sin_freq, positions_padded )

        # Calculate validation loss
        val_loss = jnp.mean((predictions - val_yields) ** 2)
        running_val_loss += val_loss.item()
 
        # Collect predictions and true labels
        all_predictions.append(np.asarray(jnp.mean(predictions, axis=1).squeeze()))  # Flatten predictions
        all_true_labels.append(np.asarray(val_yields))

        # Calculate R² score
        #r2 = r2_score( np.asarray(val_yields), np.asarray( predictions[:, -1, 0]  ) ) # Use just the last dim of pred
        r2_alt = r2_score( np.asarray(val_yields), np.asarray( jnp.mean(predictions, axis=1).squeeze()  ) )
        print("Step , MAE Loss, R2 value for current batch :", step, val_loss, r2_alt)
        running_r2_score += r2_alt
        step += 1 

    # Flatten collected lists to get overall predictions and true labels
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_true_labels = np.concatenate(all_true_labels, axis=0)
    
    # Save true labels and predictions to a .npy file for future use
    np.save("val_true_labels.npy", np.asarray(all_true_labels))
    np.save("val_predictions.npy", np.asarray(all_predictions))

    # Compute RMSE
    rmse = mean_squared_error(all_true_labels, all_predictions, squared=False)
    # Compute R² score
    r2 = r2_score(all_true_labels, all_predictions)
    # Plotting True vs. Predicted
    plot_true_vs_predicted(all_true_labels, all_predictions, rmse, r2)
    # Print RMSE and R²
    print(f"Validation RMSE: {rmse}")
    print(f"Validation R² Score: {r2}")

    return running_val_loss / len(val_loader),  running_r2_score / len(val_loader)

# After training loop
print("\nTotal number of validation batches : ",len(val_loader))
val_loss, r2_val  = validate( predictor, Mistral.mistral_model, val_loader, cache_k, cache_v, cos_freq, sin_freq, positions_padded)
print(f"[Double Check!]Validation Loss: {val_loss}")
print(f"[Double Check!] Validation R2: {r2_val}")
